{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (pytensor.tensor.blas): Using NumPy C-API based implementation for BLAS functions.\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [theta]\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40629c414b3e449da488e9a60f0bef7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 23 seconds.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        mean     sd  hdi_3%  hdi_97%  mcse_mean  mcse_sd  ess_bulk  ess_tail  \\\n",
      "theta -0.013  0.992  -1.828    1.889      0.025    0.018    1525.0    2841.0   \n",
      "\n",
      "       r_hat  \n",
      "theta    1.0  \n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import arviz as az\n",
    "import numpy as np\n",
    "\n",
    "# 간단한 모델 예제\n",
    "with pm.Model() as model:\n",
    "    theta = pm.Normal(\"theta\", mu=0, sigma=1)\n",
    "    trace = pm.sample(1000, tune=500)\n",
    "\n",
    "# 결과 요약\n",
    "print(az.summary(trace))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 81 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 93 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 94 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 104 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 88 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated C-index: 0.516\n",
      "C-index for each fold: [0.520408629755397, 0.520460795680435, 0.5164896905437558, 0.5157269106023675, 0.5050184801835526]\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data = pd.read_csv(\"./colon_gu.csv\")\n",
    "duration_col = \"stime\"\n",
    "event_col = \"event_inc\"\n",
    "\n",
    "# 지역 변수 통합 (가장 큰 값의 인덱스를 그룹으로 설정)\n",
    "data[\"region\"] = data[[\"gu_1\", \"gu_2\", \"gu_3\", \"gu_4\", \"gu_5\", \n",
    "                       \"gu_6\", \"gu_7\", \"gu_8\", \"gu_9\", \"gu_10\", \n",
    "                       \"gu_11\", \"gu_12\", \"gu_13\", \"gu_14\", \"gu_15\"]].idxmax(axis=1)\n",
    "\n",
    "# 지역 이름을 숫자로 변환 및 0부터 시작하도록 매핑\n",
    "unique_regions = {region: idx for idx, region in enumerate(data[\"region\"].unique())}\n",
    "data[\"region\"] = data[\"region\"].map(unique_regions)\n",
    "\n",
    "# 생존 시간 값이 0 이하인 경우 수정\n",
    "data[duration_col] = data[duration_col].apply(lambda x: 0.01 if x <= 0 else x)\n",
    "\n",
    "# 전체 고유 지역 수 계산\n",
    "n_regions = len(unique_regions)\n",
    "\n",
    "# Cross-validation 설정\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "c_indices = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "    # PyMC를 사용한 Frailty 모델 정의\n",
    "    with pm.Model() as frailty_model:\n",
    "        # Frailty (전체 지역 고유 수에 기반)\n",
    "        frailty = pm.Normal(\"frailty\", mu=0, sigma=1, shape=n_regions)\n",
    "        \n",
    "        # 기저 위험률 (Baseline Hazard)\n",
    "        baseline_hazard = pm.Normal(\"baseline_hazard\", mu=0, sigma=1)\n",
    "        \n",
    "        # 선형 예측값\n",
    "        log_hazard = baseline_hazard + frailty[train_data[\"region\"].values]\n",
    "        \n",
    "        # 위험률 (Hazard)\n",
    "        hazard = pm.Deterministic(\"hazard\", pm.math.exp(log_hazard))\n",
    "        \n",
    "        # 생존 시간 모델링 (Exponential distribution)\n",
    "        observed = pm.Exponential(\"observed\", lam=hazard, observed=train_data[duration_col])\n",
    "        \n",
    "        # 샘플링\n",
    "        trace = pm.sample(1000, tune=500, return_inferencedata=False, progressbar=False)\n",
    "    \n",
    "    # 테스트 데이터 예측\n",
    "    frailty_test = trace[\"frailty\"].mean(axis=0)[test_data[\"region\"].values]\n",
    "    baseline_hazard_test = trace[\"baseline_hazard\"].mean()\n",
    "    log_hazard_test = baseline_hazard_test + frailty_test\n",
    "    hazard_test = np.exp(log_hazard_test)\n",
    "\n",
    "    # C-index 계산\n",
    "    c_index = concordance_index(test_data[duration_col], -hazard_test, test_data[event_col])\n",
    "    c_indices.append(c_index)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Cross-validated C-index: {np.mean(c_indices):.3f}\")\n",
    "print(f\"C-index for each fold: {c_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 95 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 108 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 127 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 126 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [frailty, baseline_hazard]\n",
      "Sampling 4 chains for 500 tune and 1_000 draw iterations (2_000 + 4_000 draws total) took 122 seconds.\n",
      "The rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\n",
      "The effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cross-validated C-index: 0.516\n",
      "C-index for each fold: [0.5204601407738901, 0.520460795680435, 0.5164896905437558, 0.5160581412478847, 0.5050184801835526]\n"
     ]
    }
   ],
   "source": [
    "import pymc as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from lifelines.utils import concordance_index\n",
    "\n",
    "# 데이터 로드 및 전처리\n",
    "data = pd.read_csv(\"./colon_gu.csv\")\n",
    "duration_col = \"stime\"\n",
    "event_col = \"event_inc\"\n",
    "\n",
    "# 지역 변수 통합 (가장 큰 값의 인덱스를 그룹으로 설정)\n",
    "data[\"region\"] = data[[\"gu_1\", \"gu_2\", \"gu_3\", \"gu_4\", \"gu_5\", \n",
    "                       \"gu_6\", \"gu_7\", \"gu_8\", \"gu_9\", \"gu_10\", \n",
    "                       \"gu_11\", \"gu_12\", \"gu_13\", \"gu_14\", \"gu_15\"]].idxmax(axis=1)\n",
    "\n",
    "# 지역 이름을 숫자로 변환 및 0부터 시작하도록 매핑\n",
    "unique_regions = {region: idx for idx, region in enumerate(data[\"region\"].unique())}\n",
    "data[\"region\"] = data[\"region\"].map(unique_regions)\n",
    "\n",
    "# 생존 시간 값이 0 이하인 경우 수정\n",
    "data[duration_col] = data[duration_col].apply(lambda x: 0.01 if x <= 0 else x)\n",
    "\n",
    "# 전체 고유 지역 수 계산\n",
    "n_regions = len(unique_regions)\n",
    "\n",
    "# Cross-validation 설정\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "c_indices = []\n",
    "\n",
    "for train_index, test_index in kf.split(data):\n",
    "    train_data, test_data = data.iloc[train_index], data.iloc[test_index]\n",
    "\n",
    "    # PyMC를 사용한 Frailty 모델 정의\n",
    "    with pm.Model() as frailty_model:\n",
    "        # 감마 분포 기반 Frailty (양수 값으로 제한)\n",
    "        frailty = pm.Gamma(\"frailty\", alpha=2.0, beta=1.0, shape=n_regions)\n",
    "        \n",
    "        # 기저 위험률 (Baseline Hazard)\n",
    "        baseline_hazard = pm.Normal(\"baseline_hazard\", mu=0, sigma=1)\n",
    "        \n",
    "        # 선형 예측값\n",
    "        log_hazard = baseline_hazard + pm.math.log(frailty[train_data[\"region\"].values])  # 감마 분포는 로그 공간에서 사용\n",
    "        \n",
    "        # 위험률 (Hazard)\n",
    "        hazard = pm.Deterministic(\"hazard\", pm.math.exp(log_hazard))\n",
    "        \n",
    "        # 생존 시간 모델링 (Exponential distribution)\n",
    "        observed = pm.Exponential(\"observed\", lam=hazard, observed=train_data[duration_col])\n",
    "        \n",
    "        # 샘플링\n",
    "        trace = pm.sample(1000, tune=500, return_inferencedata=False, progressbar=False)\n",
    "    \n",
    "    # 테스트 데이터 예측\n",
    "    frailty_test = trace[\"frailty\"].mean(axis=0)[test_data[\"region\"].values]\n",
    "    baseline_hazard_test = trace[\"baseline_hazard\"].mean()\n",
    "    log_hazard_test = baseline_hazard_test + np.log(frailty_test)\n",
    "    hazard_test = np.exp(log_hazard_test)\n",
    "\n",
    "    # C-index 계산\n",
    "    c_index = concordance_index(test_data[duration_col], -hazard_test, test_data[event_col])\n",
    "    c_indices.append(c_index)\n",
    "\n",
    "# 결과 출력\n",
    "print(f\"Cross-validated C-index: {np.mean(c_indices):.3f}\")\n",
    "print(f\"C-index for each fold: {c_indices}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "surv_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
